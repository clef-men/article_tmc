\section{\OCaml Implementation}
\label{sec:implementation}

For reasons of space, we moved some of the content in this section to \Appendices{\cref{app:more-ocaml}}{appendices}:
%
\Appendices{\cref{subsec:alternative-impls}}{Appendix A.1}
  discusses alternative language implementation techniques that do not require TMC.
%
\Appendices{\cref{subsec:design-choices}}{Appendix A.2}
  describes how the OCaml compiler decides which calls to optimize,
  and requires mandatory disambiguation hints from the user in case of ambiguity.
%
\Appendices{\cref{subsec:PR-history}}{Appendix A.3} provides a summary of the history of our implementation (started in 2015, restarted in 2020, merged in 2021).
%
\Appendices{\cref{subsec:implementation}}{Appendix A.4} explains that implementing the transformation requires a bit of care as a naive implementation is quadratic in function size. We use an applicative functor to structure a single-pass implementation that remains nicely compact and readable.
%
\Appendices{\cref{subsec:adoption}}{Appendix A.5} surveys the adoption of the TMC transformation in the standard library,
  and in third-party \OCaml code bases, that happened since the feature was released in 2022.

\subsection{Examples}
\label{subsec:ocaml-examples}

% Many functions that consume and produce lists are
% tail-recursive-modulo-cons, in the sense that they admit a TMC
% transformation where all recursive calls become tail calls. Notable
% functions include \ocaml{map}, as already discussed, but also for
% example:

\begin{minipage}{0.47\linewidth}
\begin{Ocaml}
let[@tail_mod_cons] rec filter p =
  function
  | [] -> []
  | x :: xs ->
    if p x
    then x :: filter p xs
    else filter p xs
\end{Ocaml}
\end{minipage}
\hfill
\begin{minipage}{0.53\linewidth}
\begin{Ocaml}
let[@tail_mod_cons] rec merge cmp l1 l2 =
  match l1, l2 with
  | [], l | l, [] -> l
  | h1 :: t1, h2 :: t2 ->
      if cmp h1 h2 <= 0
      then h1 :: merge cmp t1 l2
      else h2 :: merge cmp l1 t2
\end{Ocaml}
\end{minipage}

TMC is not useful only for lists or other ``linear'' data types, with
at most one recursive occurrence of the datatype in each
constructor. An example follows.

\paragraph{A non-example} Consider a \ocaml{map} function on binary
trees:
\begin{Ocaml}
let[@tail_mod_cons] rec map f = function
| Leaf v -> Leaf (f v)
| Node(t1, t2) -> Node(map f t1, (map[@tailcall]) f t2)
\end{Ocaml}
In this function, there are two recursive calls, but only one of them
can be optimized; we used the \ocaml{[@tailcall]} attribute to direct
our implementation to optimize the call to the right child, as we will
discuss later. This is a \emph{bad} example of TMC usage in most
cases, given that
\begin{itemize}
\item If the tree is arbitrary, there is no reason that it would be
  right-leaning rather than left-leaning. Making only the right-child
  calls tail-calls does not protect us from stack overflows.
\item If the tree is known to be balanced, then in practice the depth
  is probably very small in both directions, so the TMC transformation
  is not necessary to have a well-behaved function.
\end{itemize}

\paragraph{Interesting non-linear examples} There \emph{are} interesting
examples of TMC-transformation on functions operating on tree-like
data structures, when there are natural assumptions about which child
is likely to contain a deep subtree. The \OCaml compiler itself
contains a number of them; consider for example the following function
from the \ocaml{Cmm} module, one of its lower-level program
representations:

\begin{Ocaml}
let[@tail_mod_cons] rec map_tail f = function
  | Clet(id, exp, body) ->
      Clet(id, exp, map_tail f body)
  | Cifthenelse(cond, ifso, ifnot) ->
      Cifthenelse(cond, map_tail f ifso, (map_tail[@tailcall]) f ifnot)
  | Csequence(e1, e2) ->
      Csequence(e1, map_tail f e2)
  | Cswitch(e, tbl, el) ->
      Cswitch(e, tbl, Array.map (map_tail f) el)
  [...]
\end{Ocaml}

This function is traversing the ``tail'' context of an arbitrary program term -- a meta-example!
The \ocaml{Cifthenelse} node acts as our binary-node constructor.
We do not know which side is likely to be larger, so TMC is not so interesting.
The recursive calls for \ocaml{Cswitch} are not in TMC position.
But on the other hand the \ocaml{Clet}, \ocaml{Csequence} do benefit from the TMC transformation: while they have several recursive subtrees, they are in practice only deeply nested in the direction that is turned into a tailcall by the transformation.
The \OCaml compiler does sometimes encounter machine-generated programs with a unusually long sequence of either constructions, and the TMC transformation may very well avoid a stack overflow in this case.

Another example would be \href{https://github.com/ocaml/ocaml/pull/9636}{\#9636}, a patch to the \OCaml compiler proposed in June 2020 by Mark Shinwell, to get a partially-tail-recursive implementation of the ``Common Subexpression Elimination'' (CSE) pass through a manual continuation-passing-style transform.
Xavier Leroy remarked that the existing implementation in fact fits the TMC fragment. Not all recursive calls become tail-calls (this would require a more powerful transformation or a longer, less readable patch), but the behavior of TMC on the unchanged code matches the tail-call-ness proposed in the human-written patch.

\subsection{Specifying Which Calls are in TMC Position} \label{subsec:specification}
To reason about the stack usage of their programs, users must understand which calls are in tail-modulo-cons position.
Informally, they are the calls placed under any composition of either tail-recursive or constructor contexts.

We can in fact give a simple formal description of this intuition, here for \DataLang in \cref{fig:contexts}.
A tail frame $\datalangTailFrame$ is a single term-former with holes in tail-position.
A constructor frame $\datalangConsFrame$ is a single constructor term-former (we omit deterministic blocks, which do not occur in the source).
A tail context $\datalangTailCtx$ is an arbitrary composition of tail-frame, and a TMC context $\datalangTMCCtx$ is an arbitrary composition of tail frames and constructor frames.

If a source function can be decomposed in a TMC context $\datalangTMCCtx$ with source expressions in its holes, some of which are calls to TMC-transformed functions, then our relation admits a DPS transformation where all those function calls are tail-calls, and this transformation is reachable in our \OCaml implementation, possibly by adding some annotations.

Note in particular that we do not only optimize calls to the same function we are defining, direct calls to arbitrary other functions can be transformed, if those functions have been annotated to be TMC-transformed.
This is analogous to how most functional languages support arbitrary \emph{tail calls} and not just tail self-recursion. We seamlessly support mutually recursive functions, DPS calls into locally-bound functions, etc. On the other hand, we currently do not optimize call to higher-order function arguments, or calls crossing module boundaries.

\begin{figure}[tp]
    \begin{tabular}{lclcl}
            $\datalangTailFrame[]$
            & $\ni$ &
            $\datalangTailFrame$
            & $\Coloneqq$ &
            $\datalangLet {\datalangVar} {\datalangExpr} \datalangCtxHole \mid \datalangIf \datalangExpr \datalangCtxHole \datalangCtxHole$
\\
            $\datalangConsFrame[]$
            & $\ni$ &
            $\datalangConsFrame$
            & $\Coloneqq$ &
            $\datalangBlock \datalangTag \datalangExpr \datalangCtxHole
            \mid \datalangBlock \datalangTag \datalangCtxHole \datalangExpr$
\\
            $\datalangTMCFrame[]$
            & $\ni$ &
            $\datalangTMCFrame$
            & $\Coloneqq$ &
            $\datalangTailFrame
             \mid
             \datalangConsFrame$
\\
            $\datalangTailCtx[]$
            & $\ni$ &
            $\datalangTailCtx$
            & $\Coloneqq$ &
            $\datalangCtxHole
             \mid
             \datalangTailFrame
             \mid
             \datalangTailCtx{[\datalangTailCtx]}$
\\
            $\datalangTMCCtx[]$
            & $\ni$ &
            $\datalangTMCCtx$
            & $\Coloneqq$ &
            $\datalangCtxHole
             \mid
             \datalangTMCFrame
             \mid
             \datalangTMCCtx{[\datalangTMCCtx]}$
    \end{tabular}
    \caption{\DataLang contexts for optimizable calls}
    \label{fig:contexts}
\end{figure}

\subsection{Constructor Compression} \label{subsec:constructor-compression} The translation as we described it formally in \cref{subsec:transformation} generates unpleasant code when many constructors are nested before the recursive call. For example, consider this strange function duplicating each element of a list:
\begin{Ocaml}
let rec dup = function [] -> [] | x :: xs -> x :: x :: dup xs
\end{Ocaml}

Such nested constructors are common in compiler code bases, for
example a desugaring pass that transforms a single term-former into
a composition of several simpler term-formers, and applies recursively to
its subterms.

Following the TMC transformation naively, the DPS version would propagate two different locations and performs two writes. We introduced ``constructor compression'', an optimization of the generated code that avoids creating intermediary destinations for nested constructors, leading to clearer generated code and better constant factors. Compare the naive translation of \ocaml|dup|, on the left, and our compressed translation on the right:

\begin{minipage}{0.5\linewidth}
\begin{Ocaml}
let rec dup_dps dst ofs = function
| [] -> dst.(ofs) <- []
| x :: xs ->
  let dst1 = x :: ? in
  dst.(ofs) <- dst1;
  let dst2 = x :: ? in
  dst1.(1) <- dst2;
  dup_dps dst2 1 xs
\end{Ocaml}
\end{minipage}
\hfill
\begin{minipage}{0.5\linewidth}
\begin{Ocaml}
let rec dup_dps dst ofs = function
| [] -> dst.(ofs) <- []
| x :: xs ->
  let dst2 = x :: ? in
  dst.(ofs) <- x :: dst2;
  dup_dps dst2 1 xs
\end{Ocaml}
\end{minipage}

This is implemented by passing a new transformation parameter: a stack of ``delayed'' constructor applications, that are in context and must be applied to the result of the subterm. When we encounter the final recursive call, we ``reify'' this stack: the last/innermost constructor in the stack becomes the new destination (\ocaml|dst2| in the example above), and the rest of the stack is applied to the new destination when we write to the old destination. There are two subtleties:
\begin{enumerate}
\item \ocaml|if p then e1 else e2| has two subterms which are transformed in DPS style, and naively passing the stack of delayed constructors to both subterms would duplicate code; instead we also reify the current stack when encountering such constructs. For example,\\
\ocaml{  1 :: 2 :: if p then (3 :: f ()) else (4 :: f ())} %
becomes:\\
\ocaml{  let dst1 = 1 :: 2 :: ? in}\\
\ocaml{  if p then (let dst2 = 3 :: ? in dst1.1 <- dst2; f dst2 1 ())}\\
\ocaml{       else (let dst3 = 4 :: ? in dst1.1 <- dst3; f dst3 1 ())}

\item This transformation may permute constructor applications after effectful subterms. If the constructor application context frame contains possibly-effectful subterms (for example \ocaml{f x :: _? } instead of \ocaml{x :: _?}), the compiler must \ocaml|let|-bind them at their original position to avoid changing the evaluation order. For example,\\
\ocaml{x () :: (y (); f ())} does not become \ocaml{y (); (let dst = x () :: ? in ...)},
but instead \\ \ocaml{let tmp = x () in (y (); let dst = tmp :: ? in ...)}.
\end{enumerate}

It is conceptually easy to extend our previous formalization of TMC as a rewriting relation to capture constructor compression, by indexing this relation on an additional list of constructor contexts. We do not present this here for lack of space, but included this change in our \Coq proofs, which establish correctness of TMC in presence of constructor compression.

\subsection{Evaluation: Benchmarks}

We measured the performance of \ocaml|List.map (fun n -> n + 1)| to validate our claims that the TMC transformation preserves program performance, and lets us replace complex hand-optimized tail-recursive implementations. \ocaml|List.map| is a worst-case: with most of the time spent in recursion and list construction, it is more sensitive to constant-factor overheads than other recursive functions.

\begin{figure}[tp]
\def\svgscale{0.8}
\graphicspath{{plots/}}
\input{plots/plot.5.pdf_tex}
\caption{\ocaml|List.map| benchmark on \OCaml~5.1}
\label{fig:bench5}
\end{figure}

\newcommand{\bench}[1]{\textbf{#1}}

The different versions we benchmark are the following. We measure the code size (in lines) of each version, as a reasonable approximation of its implementation complexity.
\begin{description}
\item[\bench{nontail}] (5 lines of code) The naive, non-tail-recursive implementation.
\item[\bench{tail}] (9 lines) The naive tail-recursive implementation,
  \ocaml{List.rev (List.rev_map f xs)}.
\item[\bench{base}] (78 lines) The implementation of Jane Street's
  \href{https://github.com/janestreet/base}{Base} library
  (version 0.14.0). It is heavily hand-optimized to compensate for the costs
  of being tail-recursive.
\item[\bench{containers}] (55 lines) Another standard-library extension by Simon
  Cruanes; it is the hand-optimized tail-recursive implementation we
  included in the Prologue.
\item[\bench{batteries}] (29 lines) The implementation of the community-maintained
  \href{https://github.com/ocaml-batteries-team/batteries-included/}{Batteries}
  library. It is actually written in destination-passing-style, using
  an unsafe encoding with \ocaml{Obj.magic} to unsafely cast a mutable
  record into a list cell. (The trick comes from the older Extlib
  library, was introduced by Brian Hurt in 2003, and has a comment crediting Jacques
  Garrigue for the particular encoding used.)
\item[\bench{tmc}] (5 lines) ``Our'' version, the last version of the Prologue: the
  result of applying our implementation of the TMC transformation to
  the simple, non-tail-recursive version.
\item[\bench{tmc-unrolled}] (18 lines) The result of manually unrolling the \bench{tmc} implementation three times, to be compared with \bench{base} and \bench{containers} that use manual unrolling as well.
\end{description}

The benchmarks reports the relative performance compared to the naive tail-recursive version as our baseline. They were run on \OCaml~5.1 in July 2024, on a Linux machine with an AMD Ryzen processor fixed at a 3Ghz frequency, looping each measurement for 5s (a single \ocaml|List.map| run takes between 7ns, for empty lists, and 89ms on lists with a million element).

Qualitatively we see that there are four groups:
\begin{itemize}
\item \bench{tmc}, \bench{batteries} perform very well on large lists, but they
  are slower than the baseline on small lists.
\item \bench{nontail} performs better than \bench{tmc}, \bench{batteries} on list sizes up to $10^4$, and much worse on larger lists.
\item \bench{base}, \bench{containers} perform noticeably better than \bench{nontail} at all sizes,
  but worse than the TMC versions above size $10^4$.
\item \bench{tmc-unrolled} is the best option: it performs as well as \bench{base} and \bench{containers} before $10^4$, and as well as \bench{tmc}, \bench{batteries} afterwards.
\end{itemize}
Our interpretation of the result is that some unrolling makes a noticeable performance difference for such a short function: \bench{tmc} is not good enough on smaller lists, but \bench{tmc-unrolled} is the best-performing, despite being much simpler than the \bench{base} and \bench{containers} versions.

\paragraph{Asymptotics of \bench{nontail}} The bad behavior of \bench{nontail} on large lists comes from a quadratic behavior on very large call stacks, coming from a repeated scan of the call stack during minor collections. (The \OCaml compiler and runtime could be tweaked to avoid this quadratic behavior, at the cost of some small constant overhead on function returns.)

% \paragraph{Memory usage} The \bench{tail} version allocates twice as many words as the \bench{tail} or TMC versions. The \bench{base} and \bench{containers} version allocate more as well, once they reach the cutoff where they move to their tail-recursive regime. This probably does not matter for most use-cases, and it does not show in the time measurements in the microbenchmark.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End: